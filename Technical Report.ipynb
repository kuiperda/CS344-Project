{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving puzzles from The Witness - CS 344 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Daniel Kuiper and Bernard Boadu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we train a system to be more intuitive when making a decision? In other words, how can a system take what it has learned from a sample training data and adapt to a new environment? For example, could a chess AI system play 4-player chess well without training on that variation? Can a model train on a set of simple puzzles and still do well when generalizing to more complex examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question is interesting because humans learn to do this quickly - someone who is good at chess is in a good place to win at 4-player chess even on their first time playing. There are some different rules/patterns of the game, but a human knows which old rules are still relevant (queens are still useful) and can already guess how they will have to play differently with two extra players to deal with. (maybe trading pieces is less appealing) Something that humans can do intuitively should be possible for a model to learn to do, so we wanted to find out what that would look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generated our own dataset by having the program create puzzles and then solve them using a bruteforce approach. We converted the matrix into an image like the one we have below. The data contains all possible valid finished and unfinished states of a path traversing the grid. We can generate a lot of unique and interesting data quickly which is one strength, but our data is limited to 3x3 or less puzzles since the time to do this for a 4x4 is a little inconvenient and a 5x5 would be very inconvenient (taking hours to generate). Additionally, our examples are not exhaustive- we don't generate every possible puzzle, just a few randomly generated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[['v', 'e', 'v', 'e', 'v', 'e', 'f'],\n",
    " ['e', ' ', 'e', ' ', 'e', ' ', 'e'],\n",
    " ['v', 'e', 'v', 'e', 'v', 'e', 'v'],\n",
    " ['e', ' ', 'e', ' ', 'e', ' ', 'e'],\n",
    " ['X', 'e', 'v', 'e', 'X', 'e', 'X'],\n",
    " ['e', ' ', 'e', ' ', 'e', ' ', 'e'],\n",
    " ['b', 'e', 'v', 'e', 'v', 'e', 'v']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test_image_subsets/finished/grid1005.png\" width=\"200\" height=\"200\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model that can solve puzzles from the witness- particularly ones it hasn't seen before, that are more complex than the ones it trained off of. More specifically, train a model on 2x2 or 3x3 dotted puzzles (you must cross certain vertices for finished states to count as solving the puzzle) and see if it performs well on a 5x5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Goal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model to predict whether a subpath would most likely result in a solved state or not. Thereafter, this prediction would be used in a search tree to decide the next path to take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Goal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time constraints, we pivoted to training a model that would determine if a grid of the game was either solved, finished or unfinished.\n",
    "\n",
    "* Solved = Puzzle made it to the end and followed all the rules (i.e., collected all the dots)\n",
    "* Finished = Puzzle made it to the end but failed to follow all the rules (missed some dots)\n",
    "* Unfinished = Puzzle did not make it to the end at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recognized that in generating our data, solved states were much less common than unfinished states. When we made grids with a lower percentage of dots, this was offset a little - fewer required vertices means more unique paths that solve the puzzle. Finished states were more (~2-3x as many) than solved, but still much less than unfinished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model: CNN using Resnet18 (with fast.ai providing the default loss function)\n",
    "* Targets: 'Solved', 'Finished' and 'Unfinished'\n",
    "* Metrics: Error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into a Train(80%) and Validation set(20%). Thereafter, we tested our model using an entirely new grid outside the original train-valid set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline result using a decision tree overfitted because we did include the grid puzzle in the data (i.e., only used subpaths generated). Also, we failed to split the initial dataset into a train-valid set. So this performed well at recognizing the data we gave it but was not a good way to solve our problem since it could not generalize at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempts at improved results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Included the grid puzzle in the training process and split the dataset into a train-valid set. We also converted our grids into images to put into the CNN. The results were a lot more relevant to what we were doing, so they were better. The CNN recognized the different states well even on data it hadn't seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our confusion matrix, we noticed that 10 finished grid states were misclassified as solved. This might be due to the fewer solved/finished grid states in the dataset compared to unfinished grid states.Another reason for this misclassification would be that both the solved and finished grid states look similar. One thing we noticed was that our model didn't do well on 2x2 grids, which would be because we trained it only on 3x3 grids. If we had included 2x2 data, it would have done better and also could have generalized better to higher dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of effects of alternative choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative choice would have been to have used the entire unfinished grid states (rather than a subset)- from the 6 grids we generated for test data, we got ~220 solved states and ~6,000 unfinished states. We chose to take only a subset (~800) of the latter. Our choice was tied to the fact that we believed the dataset would be imbalanced and would overfit on the unfinished grid states. Another approach would be to increase the number of solved grid states to see if the number of misclassification of solved and unfinished grid states would reduce. We also could have tried a transformer architecture which would probably have been a better approach - predicting the next step to take in a sequence (our path)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that it takes a long time to generate your own data and then to label it correctly for your task. A data-generating program runs quickly but takes a long time to set up. We found that our initial goal was more in-depth than we had time for and our initial approach was not the right way to go. We found, though, that casting the problem as an image classification problem gave good results for a smaller problem (recognizing fully solved states). The classifier seemed to have learned the rules behind the puzzle since it recognized solved states on grids it hadn't seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to achieve our final goal (i.e.learning the difference between different states) but were unable to achieve our initial goal due to limitations with data and time. We still don't have a system that intuits and generalizes to new situations very well, and it doesn't solve the puzzles, only recognizes solved puzzles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and future directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Limitations: insufficient data (our format would need to change for real reinforcement learning), time. This classifier only recognizes one semi-interesting puzzle type, and not any others (though we could add more puzzle types without too much difficulty)\n",
    "* Future directions: a different type of dataset would be needed to train a model that could really solve the puzzles. Getting it to generalize to larger and more complex grids with multiple puzzle types would be a good goal to reach. Improving our data (changing it to a regression problem that recognizes how far paths are from being solved so it can evaluate the state) or changing to a transformer architecture are two suggestions. \n",
    "Also, if you found our idea interesting at all, and you enjoy this type of thing, I highly recommend getting the game."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
